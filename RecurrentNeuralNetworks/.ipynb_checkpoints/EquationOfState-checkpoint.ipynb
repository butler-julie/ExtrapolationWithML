{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM LEVEL IMPORTS\n",
    "import sys\n",
    "\n",
    "# THIRD-PARTY IMPORTS\n",
    "# For matrices and calculations\n",
    "import numpy as np\n",
    "# For machine learning (backend for keras)\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_v2_behavior\n",
    "# User-friendly machine learning library\n",
    "# Front end for TensorFlow\n",
    "import keras \n",
    "# Different methods from Keras needed to create an RNN\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "# For graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing the import directory\n",
    "sys.path.append('../DataSets/')\n",
    "# LOCAL IMPORTS\n",
    "# Encoded data sets but can apply this code to any data set\n",
    "#from Datasets import *\n",
    "from EquationOfState import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a string that represents the name of the data set, a recommended training \n",
    "# dimension for the data, the total x data, and the total y data for a data set\n",
    "# the is encoded in the file DataSets.py\n",
    "# This code can be used with other data sets as long as a training dimension is supplied\n",
    "# with the name \"dim\", the x data is in a one dimensional numpy array named \"X_tot\", and the\n",
    "# y data is in a one dimensional numpy array called \"y_tot\".\n",
    "X_tot, y_tot, design_matrix = EquationOfState()\n",
    "dim = 80\n",
    "# Check to see if the data set is complete\n",
    "assert len(X_tot) == len(y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data, length_of_sequence = 2):  \n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            data(a numpy array): the data that will be the inputs to the recurrent neural\n",
    "                network\n",
    "            length_of_sequence (an int): the number of elements in one iteration of the\n",
    "                sequence patter.  For a function approximator use length_of_sequence = 2.\n",
    "        Returns:\n",
    "            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its\n",
    "                dimensions are length of data - length of sequence, length of sequence, \n",
    "                dimnsion of data\n",
    "            rnn_output (a numpy array): the training data for the neural network\n",
    "        Formats data to be used in a recurrent neural network.  The resulting data points have the\n",
    "        following format for a sequence length of n: ((y1, y2, ..., yn), yn+1).  This function is \n",
    "        adapted from the one found here: \n",
    "        https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "    \"\"\"\n",
    "    # To store the formated \"x\" and \"y\" data\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data)-length_of_sequence):\n",
    "        # Get the next length_of_sequence elements\n",
    "        a = data[i:i+length_of_sequence]\n",
    "        # Get the element that immediately follows that\n",
    "        b = data[i+length_of_sequence]\n",
    "        # Reshape so that each data point is contained in its own array\n",
    "        a = np.reshape (a, (len(a), 1))\n",
    "        X.append(a)\n",
    "        Y.append(b)\n",
    "    # Convert into numpy arrays as these are easier to use later in the code.\n",
    "    rnn_input = np.array(X)\n",
    "    rnn_output = np.array(Y)\n",
    "    return rnn_input, rnn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(length_of_sequences, batch_size = None, stateful = False):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            length_of_sequence (an int): the length of sequence used to format the training \n",
    "                data (i.e. the length of the sequence used in format_data).\n",
    "            batch_size (an int): See Keras documentation for Input\n",
    "            stateful (a boolean): See Keras documentation for SimpleRNN\n",
    "        Returns:\n",
    "            model (a Keras model): the build and compiled recurrent neural network\n",
    "        Creates a simple recurrent neural network with one simple recurrent hidden layer with\n",
    "        200 hidden neurons and compiles the network using a mean-squared error loss function \n",
    "        and an Adam's optimizer.\n",
    "    \"\"\"\n",
    "    # Number of neurons in the input and output layer\n",
    "    in_out_neurons = 1\n",
    "    # Number of neurons in the hidden layer\n",
    "    hidden_neurons = 200\n",
    "    # Create the input layer\n",
    "    inp = Input(batch_shape=(batch_size, \n",
    "                length_of_sequences, \n",
    "                in_out_neurons))  \n",
    "    # Create the simple recurrent hidden layer\n",
    "    rnn = SimpleRNN(hidden_neurons, \n",
    "                    return_sequences=False,\n",
    "                    stateful = stateful,\n",
    "                    name=\"RNN\")(inp)\n",
    "    # Create a dense (feedforward) neural network layer which will act as the output layer\n",
    "    dens = Dense(in_out_neurons,name=\"dense\")(rnn)\n",
    "    # Build the model\n",
    "    model = Model(inputs=[inp],outputs=[dens])\n",
    "    # Compile the model using the specified loss function and optimizer\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data for the RNN using a sequence length of 2\n",
    "# Get the first dim points from the total data set to use for training\n",
    "X_train = X_tot[:dim]\n",
    "y_train = y_tot[:dim]\n",
    "# Formating the y component of the training data using the time series forecasting \n",
    "rnn_input, rnn_training = format_data(y_train, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2, 1)]            0         \n",
      "_________________________________________________________________\n",
      "RNN (SimpleRNN)              (None, 200)               40400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 40,601\n",
      "Trainable params: 40,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## use the default values for batch_size, stateful\n",
    "model = rnn(length_of_sequences = rnn_input.shape[1])\n",
    "# Print a summary of the Keras model to the console\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 269321.6875 - val_loss: 1787704.7500\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 268869.4688 - val_loss: 1786219.2500\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 268430.9062 - val_loss: 1784712.2500\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 267999.1562 - val_loss: 1783205.5000\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 267535.6562 - val_loss: 1781740.6250\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 267101.5625 - val_loss: 1780279.0000\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 266682.2500 - val_loss: 1778814.7500\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 266238.4062 - val_loss: 1777364.0000\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 265826.1562 - val_loss: 1775906.3750\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 265372.1562 - val_loss: 1774469.2500\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 264970.5312 - val_loss: 1772994.6250\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 264536.2500 - val_loss: 1771558.0000\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 264096.3125 - val_loss: 1770140.7500\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 263687.0938 - val_loss: 1768696.6250\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 263266.8125 - val_loss: 1767247.0000\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 262819.5625 - val_loss: 1765783.7500\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 262415.2812 - val_loss: 1764295.0000\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 261973.1875 - val_loss: 1762843.5000\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 261556.7344 - val_loss: 1761389.6250\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 261120.9219 - val_loss: 1759983.7500\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 260712.1875 - val_loss: 1758593.5000\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 260295.6250 - val_loss: 1757202.2500\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 259908.4531 - val_loss: 1755783.7500\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 259481.5625 - val_loss: 1754395.5000\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 259069.7344 - val_loss: 1752996.8750\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 258657.4531 - val_loss: 1751572.3750\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 258255.5156 - val_loss: 1750128.2500\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 257823.1562 - val_loss: 1748695.7500\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 257418.0000 - val_loss: 1747237.3750\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 256992.8125 - val_loss: 1745796.6250\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 256590.5938 - val_loss: 1744360.2500\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 256161.6250 - val_loss: 1742961.0000\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 255744.8594 - val_loss: 1741544.1250\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 255337.8594 - val_loss: 1740089.5000\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 254936.8906 - val_loss: 1738649.1250\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 254516.2656 - val_loss: 1737260.2500\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 254108.7344 - val_loss: 1735893.1250\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 253715.6562 - val_loss: 1734534.8750\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 253321.0469 - val_loss: 1733188.3750\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 252932.8125 - val_loss: 1731840.2500\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 252536.2500 - val_loss: 1730483.8750\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 252145.5469 - val_loss: 1729097.5000\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 251741.8125 - val_loss: 1727706.2500\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 251330.5469 - val_loss: 1726305.0000\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 250943.0312 - val_loss: 1724868.5000\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 250539.1562 - val_loss: 1723453.2500\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 250123.1094 - val_loss: 1722075.0000\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 249722.7500 - val_loss: 1720671.1250\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 249332.1562 - val_loss: 1719243.7500\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 248915.6562 - val_loss: 1717843.3750\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 248529.5156 - val_loss: 1716442.8750\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 248107.7344 - val_loss: 1715087.1250\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 247730.4844 - val_loss: 1713695.0000\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 247339.4375 - val_loss: 1712292.8750\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 246925.4375 - val_loss: 1710905.0000\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 246538.0000 - val_loss: 1709481.5000\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 246124.5938 - val_loss: 1708064.0000\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 245729.4062 - val_loss: 1706660.3750\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 245325.9219 - val_loss: 1705271.0000\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 244919.1406 - val_loss: 1703891.7500\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 244550.7344 - val_loss: 1702483.0000\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 244141.0781 - val_loss: 1701102.2500\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 243754.9688 - val_loss: 1699731.5000\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 243386.7344 - val_loss: 1698354.5000\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 242975.0312 - val_loss: 1697005.2500\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 242594.5156 - val_loss: 1695640.2500\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 242211.2500 - val_loss: 1694273.2500\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 241825.0781 - val_loss: 1692910.3750\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 241427.9688 - val_loss: 1691547.0000\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 241049.4531 - val_loss: 1690159.2500\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 240668.7812 - val_loss: 1688768.7500\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 240268.6562 - val_loss: 1687416.0000\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 239883.6719 - val_loss: 1686071.5000\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 239505.9531 - val_loss: 1684721.1250\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 239133.2969 - val_loss: 1683353.5000\n",
      "Epoch 76/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step - loss: 238737.7812 - val_loss: 1681995.5000\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 238371.7344 - val_loss: 1680624.2500\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 237978.4062 - val_loss: 1679252.6250\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 237625.4062 - val_loss: 1677855.1250\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 237209.4375 - val_loss: 1676520.0000\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 236836.9219 - val_loss: 1675182.1250\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 236458.4062 - val_loss: 1673832.2500\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 236091.1562 - val_loss: 1672470.3750\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 235703.2188 - val_loss: 1671129.2500\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 235337.9219 - val_loss: 1669784.0000\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 234973.3281 - val_loss: 1668456.3750\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 234590.1406 - val_loss: 1667155.2500\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 234237.6562 - val_loss: 1665825.7500\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 233852.8125 - val_loss: 1664506.1250\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 233492.7031 - val_loss: 1663164.8750\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 233118.1094 - val_loss: 1661816.2500\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 232731.3438 - val_loss: 1660473.6250\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 232372.8594 - val_loss: 1659093.0000\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 231998.2969 - val_loss: 1657721.8750\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 231605.4062 - val_loss: 1656385.0000\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 231253.0312 - val_loss: 1655034.2500\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 230864.7344 - val_loss: 1653690.5000\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 230506.5625 - val_loss: 1652330.5000\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 230125.0469 - val_loss: 1650998.0000\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 229764.9688 - val_loss: 1649659.5000\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 229384.4062 - val_loss: 1648330.2500\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 229045.6562 - val_loss: 1646980.5000\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 228642.2656 - val_loss: 1645668.7500\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 228281.0781 - val_loss: 1644345.8750\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 227931.0000 - val_loss: 1643016.2500\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 227566.3750 - val_loss: 1641699.5000\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 227210.8438 - val_loss: 1640385.7500\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 226844.1094 - val_loss: 1639089.0000\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 226492.0000 - val_loss: 1637780.5000\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 226142.9375 - val_loss: 1636450.5000\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 225783.1094 - val_loss: 1635120.5000\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 225392.1719 - val_loss: 1633811.2500\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 225038.7344 - val_loss: 1632475.2500\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 224690.8281 - val_loss: 1631127.8750\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 224321.3438 - val_loss: 1629795.3750\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 223958.8906 - val_loss: 1628476.0000\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 223594.2969 - val_loss: 1627154.8750\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 223246.2812 - val_loss: 1625817.5000\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 222868.0000 - val_loss: 1624504.5000\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 222516.7969 - val_loss: 1623182.5000\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 222154.6719 - val_loss: 1621879.8750\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 221821.0000 - val_loss: 1620570.2500\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 221456.2969 - val_loss: 1619274.6250\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 221096.5469 - val_loss: 1617977.7500\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 220759.8125 - val_loss: 1616655.3750\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 220403.6875 - val_loss: 1615356.8750\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 220048.2500 - val_loss: 1614072.5000\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 219704.0312 - val_loss: 1612761.1250\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 219346.3281 - val_loss: 1611440.5000\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 218995.0000 - val_loss: 1610116.1250\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 218628.0469 - val_loss: 1608813.2500\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 218294.5938 - val_loss: 1607500.8750\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 217937.8594 - val_loss: 1606198.5000\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 217583.7344 - val_loss: 1604892.3750\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 217236.6719 - val_loss: 1603579.1250\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 216876.4531 - val_loss: 1602273.7500\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 216532.5469 - val_loss: 1600945.1250\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 216183.1094 - val_loss: 1599614.5000\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 215845.1562 - val_loss: 1598299.1250\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 215477.3281 - val_loss: 1597019.6250\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 215134.1875 - val_loss: 1595737.0000\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 214806.0781 - val_loss: 1594452.0000\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 214457.1562 - val_loss: 1593202.5000\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 214126.8750 - val_loss: 1591960.2500\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 213802.4219 - val_loss: 1590724.0000\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 213470.0000 - val_loss: 1589504.2500\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 213143.9688 - val_loss: 1588289.3750\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 212815.3906 - val_loss: 1587066.8750\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 212493.8594 - val_loss: 1585824.7500\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 212169.7188 - val_loss: 1584579.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 211832.8438 - val_loss: 1583333.1250\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 211501.7812 - val_loss: 1582052.3750\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 211162.2812 - val_loss: 1580748.0000\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 210810.1094 - val_loss: 1579403.2500\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 210464.0000 - val_loss: 1578021.0000\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 210101.4688 - val_loss: 1576664.5000\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 209751.2656 - val_loss: 1575351.3750\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 209409.0156 - val_loss: 1574068.0000\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 209065.0938 - val_loss: 1572806.7500\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 208737.7812 - val_loss: 1571566.2500\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 208406.8438 - val_loss: 1570330.7500\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 208079.8125 - val_loss: 1569074.5000\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 207759.1875 - val_loss: 1567795.3750\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 207417.3281 - val_loss: 1566536.0000\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 207086.2344 - val_loss: 1565265.5000\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 206761.4062 - val_loss: 1564001.0000\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 206450.0312 - val_loss: 1562749.3750\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 206094.7031 - val_loss: 1561524.5000\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 205787.7500 - val_loss: 1560235.2500\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 205448.5938 - val_loss: 1558948.6250\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 205106.9219 - val_loss: 1557653.7500\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 204773.4688 - val_loss: 1556343.0000\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 204451.8594 - val_loss: 1555028.8750\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 204093.2031 - val_loss: 1553747.1250\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 203759.4062 - val_loss: 1552459.2500\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 203444.3438 - val_loss: 1551156.5000\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 147806.43 - 0s 7ms/step - loss: 203090.2344 - val_loss: 1549892.7500\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 202759.2969 - val_loss: 1548627.5000\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 202452.7188 - val_loss: 1547364.8750\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 202115.9062 - val_loss: 1546145.2500\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 201803.7969 - val_loss: 1544891.7500\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 201483.8906 - val_loss: 1543627.0000\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 201144.5625 - val_loss: 1542379.6250\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 200826.4062 - val_loss: 1541107.0000\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 200502.4062 - val_loss: 1539824.5000\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 200177.8438 - val_loss: 1538551.0000\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 199858.5469 - val_loss: 1537291.8750\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 199531.1562 - val_loss: 1536049.2500\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 199203.3438 - val_loss: 1534807.7500\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 198897.9219 - val_loss: 1533556.5000\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 198576.4062 - val_loss: 1532328.0000\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 198271.3438 - val_loss: 1531109.7500\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 197947.9531 - val_loss: 1529914.5000\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 197643.0469 - val_loss: 1528719.0000\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 197334.4062 - val_loss: 1527509.1250\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 197039.6406 - val_loss: 1526300.1250\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 196715.6250 - val_loss: 1525102.8750\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 196418.2188 - val_loss: 1523867.6250\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 196112.2656 - val_loss: 1522628.8750\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 195796.8594 - val_loss: 1521414.5000\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 195489.6875 - val_loss: 1520226.3750\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 195179.0469 - val_loss: 1519050.5000\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 194890.2812 - val_loss: 1517848.0000\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 194582.7031 - val_loss: 1516646.1250\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 194291.4062 - val_loss: 1515434.7500\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 193969.0469 - val_loss: 1514253.2500\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 193668.3750 - val_loss: 1513056.0000\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 193382.5625 - val_loss: 1511854.0000\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 193066.5469 - val_loss: 1510676.3750\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 192764.6094 - val_loss: 1509482.0000\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 192471.6094 - val_loss: 1508250.1250\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 192157.8438 - val_loss: 1507025.3750\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 191844.4531 - val_loss: 1505793.2500\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 191529.8906 - val_loss: 1504527.2500\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 191226.1406 - val_loss: 1503242.6250\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 190905.5156 - val_loss: 1501974.1250\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 190597.5625 - val_loss: 1500717.8750\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 190272.3594 - val_loss: 1499496.0000\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 189974.7969 - val_loss: 1498270.0000\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 189658.2344 - val_loss: 1497073.6250\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 189369.9375 - val_loss: 1495859.5000\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 189061.5156 - val_loss: 1494655.8750\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 188757.9219 - val_loss: 1493443.5000\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 188455.9219 - val_loss: 1492218.0000\n",
      "Epoch 225/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 188149.3750 - val_loss: 1490990.8750\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 187854.4062 - val_loss: 1489752.3750\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 187560.0000 - val_loss: 1488514.3750\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 187234.4219 - val_loss: 1487320.7500\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 186922.8750 - val_loss: 1486112.7500\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 186641.1406 - val_loss: 1484866.7500\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 186340.2188 - val_loss: 1483626.7500\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 186005.5000 - val_loss: 1482415.2500\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 185722.3281 - val_loss: 1481157.2500\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 185415.3281 - val_loss: 1479897.3750\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 185099.9844 - val_loss: 1478675.5000\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 184804.1094 - val_loss: 1477482.0000\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 184518.8125 - val_loss: 1476298.5000\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 184226.8438 - val_loss: 1475135.5000\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 183951.6719 - val_loss: 1473994.7500\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 183658.0781 - val_loss: 1472887.6250\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 183391.5156 - val_loss: 1471755.7500\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 183114.0156 - val_loss: 1470615.0000\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 182825.4062 - val_loss: 1469484.1250\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 182538.8125 - val_loss: 1468330.5000\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 182263.9688 - val_loss: 1467141.2500\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 181968.7500 - val_loss: 1465950.8750\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 181680.1094 - val_loss: 1464736.5000\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 181391.6562 - val_loss: 1463517.3750\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 181091.0469 - val_loss: 1462321.0000\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 180812.7031 - val_loss: 1461135.5000\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 180536.1562 - val_loss: 1459972.5000\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 180233.9688 - val_loss: 1458881.2500\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 179957.6094 - val_loss: 1457769.6250\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 179683.5156 - val_loss: 1456615.5000\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 179404.8750 - val_loss: 1455403.5000\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 179135.0469 - val_loss: 1454191.0000\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 178827.8750 - val_loss: 1453037.2500\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 178556.9844 - val_loss: 1451881.1250\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 178264.0312 - val_loss: 1450744.5000\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 177987.3125 - val_loss: 1449621.6250\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 177716.4688 - val_loss: 1448500.2500\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 177432.0000 - val_loss: 1447325.0000\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 177171.5000 - val_loss: 1446112.0000\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 176892.2656 - val_loss: 1444936.5000\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 176598.2656 - val_loss: 1443806.1250\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 176324.7031 - val_loss: 1442679.5000\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 176058.3281 - val_loss: 1441538.5000\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 175777.5156 - val_loss: 1440400.8750\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 175503.5000 - val_loss: 1439255.2500\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 175230.6094 - val_loss: 1438091.5000\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 174950.8594 - val_loss: 1436936.7500\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 174673.0000 - val_loss: 1435785.3750\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 174402.2500 - val_loss: 1434636.0000\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 174123.3594 - val_loss: 1433497.7500\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 173853.5625 - val_loss: 1432362.6250\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 173601.8906 - val_loss: 1431232.0000\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 173327.5156 - val_loss: 1430130.0000\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 173065.2812 - val_loss: 1429020.2500\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 172797.6250 - val_loss: 1427900.5000\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 172522.5781 - val_loss: 1426799.0000\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 172276.2031 - val_loss: 1425686.0000\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 172004.8906 - val_loss: 1424609.6250\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 171742.4531 - val_loss: 1423516.0000\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 171491.5156 - val_loss: 1422377.0000\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 171223.0625 - val_loss: 1421255.0000\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 170962.4375 - val_loss: 1420152.6250\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 170693.2500 - val_loss: 1419065.7500\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 170436.3281 - val_loss: 1417979.5000\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 170175.2344 - val_loss: 1416892.2500\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 169926.4062 - val_loss: 1415776.0000\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 169668.0312 - val_loss: 1414655.3750\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 169395.9531 - val_loss: 1413564.3750\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 169124.2031 - val_loss: 1412483.1250\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 168877.2344 - val_loss: 1411352.6250\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 168614.8906 - val_loss: 1410206.6250\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 168347.1562 - val_loss: 1409066.1250\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 168078.5469 - val_loss: 1407936.1250\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 167826.1094 - val_loss: 1406807.5000\n",
      "Epoch 299/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 167557.6719 - val_loss: 1405693.1250\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 167292.2500 - val_loss: 1404582.5000\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 167019.8125 - val_loss: 1403457.1250\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 166761.9531 - val_loss: 1402303.1250\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 166501.1406 - val_loss: 1401183.3750\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 166247.8125 - val_loss: 1400091.7500\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 165980.4375 - val_loss: 1399020.8750\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 165726.7500 - val_loss: 1397937.2500\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 165479.0469 - val_loss: 1396829.0000\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 165228.8125 - val_loss: 1395715.5000\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 164966.7031 - val_loss: 1394633.7500\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 164724.9688 - val_loss: 1393554.0000\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 164464.1562 - val_loss: 1392478.6250\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 164219.7031 - val_loss: 1391379.3750\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 163944.4688 - val_loss: 1390302.7500\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 163709.5469 - val_loss: 1389179.0000\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 163455.0625 - val_loss: 1388067.8750\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 163176.0469 - val_loss: 1386990.2500\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 162952.1250 - val_loss: 1385868.6250\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 162671.9531 - val_loss: 1384776.6250\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 162429.0625 - val_loss: 1383654.1250\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 162164.3594 - val_loss: 1382539.0000\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 161926.9219 - val_loss: 1381405.1250\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 161651.3281 - val_loss: 1380327.6250\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 161396.8125 - val_loss: 1379236.0000\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 161157.3438 - val_loss: 1378104.5000\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 160888.8906 - val_loss: 1376977.6250\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 160635.8438 - val_loss: 1375850.0000\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 160382.6875 - val_loss: 1374730.8750\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 160138.6562 - val_loss: 1373643.7500\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 159878.4375 - val_loss: 1372600.7500\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 159633.1875 - val_loss: 1371550.2500\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 159402.0781 - val_loss: 1370475.3750\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 159153.6250 - val_loss: 1369393.5000\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 158911.0000 - val_loss: 1368309.0000\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 158662.5625 - val_loss: 1367258.1250\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 158424.0469 - val_loss: 1366215.3750\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 158184.2344 - val_loss: 1365162.8750\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 157939.5938 - val_loss: 1364110.2500\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 157693.1562 - val_loss: 1363049.1250\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 157461.5000 - val_loss: 1361964.2500\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 157227.1562 - val_loss: 1360871.6250\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 156960.0156 - val_loss: 1359789.7500\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 156724.2969 - val_loss: 1358661.0000\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 156469.7188 - val_loss: 1357550.6250\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 156214.5000 - val_loss: 1356447.5000\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 155966.9062 - val_loss: 1355343.2500\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 155712.2656 - val_loss: 1354276.7500\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 155484.8125 - val_loss: 1353206.2500\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 155238.5781 - val_loss: 1352156.0000\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 155002.6562 - val_loss: 1351097.3750\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 154759.4375 - val_loss: 1350043.5000\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 154518.7969 - val_loss: 1349001.0000\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 154287.0469 - val_loss: 1347925.7500\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 154047.6719 - val_loss: 1346834.8750\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 153802.9219 - val_loss: 1345757.2500\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 153557.9375 - val_loss: 1344692.5000\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 153330.2188 - val_loss: 1343639.5000\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 153089.9844 - val_loss: 1342604.7500\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 152852.1250 - val_loss: 1341553.0000\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 152612.0938 - val_loss: 1340492.2500\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 152382.3750 - val_loss: 1339414.5000\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 152142.7031 - val_loss: 1338360.0000\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 151903.0469 - val_loss: 1337315.3750\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 151679.2344 - val_loss: 1336258.1250\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 151443.9375 - val_loss: 1335221.8750\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 151224.0312 - val_loss: 1334201.1250\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 150981.0156 - val_loss: 1333197.2500\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 150754.0469 - val_loss: 1332170.2500\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 150536.0000 - val_loss: 1331125.2500\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 150305.8594 - val_loss: 1330090.0000\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 150067.5938 - val_loss: 1329066.0000\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 149838.2500 - val_loss: 1328033.5000\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 149599.7500 - val_loss: 1326979.0000\n",
      "Epoch 373/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step - loss: 149373.9844 - val_loss: 1325886.8750\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 149128.2969 - val_loss: 1324784.7500\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 148885.2500 - val_loss: 1323676.5000\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 148643.9375 - val_loss: 1322573.5000\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 148404.0938 - val_loss: 1321492.7500\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 148161.1562 - val_loss: 1320430.7500\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 147936.1406 - val_loss: 1319359.0000\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 147679.0000 - val_loss: 1318301.6250\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 147449.9688 - val_loss: 1317227.0000\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 147213.1094 - val_loss: 1316144.5000\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 146978.5781 - val_loss: 1315057.2500\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 146749.0469 - val_loss: 1313999.7500\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 146508.3438 - val_loss: 1312963.5000\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 146299.0469 - val_loss: 1311915.0000\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 146056.2812 - val_loss: 1310914.6250\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 145836.4844 - val_loss: 1309892.0000\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 145604.6719 - val_loss: 1308869.5000\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 145384.8281 - val_loss: 1307831.2500\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 145166.3594 - val_loss: 1306783.2500\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 144936.7031 - val_loss: 1305744.6250\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 144718.0938 - val_loss: 1304724.1250\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 144481.9531 - val_loss: 1303728.0000\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 144272.7812 - val_loss: 1302712.8750\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 144071.2344 - val_loss: 1301702.2500\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 143834.2344 - val_loss: 1300714.0000\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 143615.1562 - val_loss: 1299700.7500\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 143409.6094 - val_loss: 1298700.0000\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 143177.7188 - val_loss: 1297719.1250\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 142965.7969 - val_loss: 1296719.7500\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 142747.4375 - val_loss: 1295736.5000\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 142538.2344 - val_loss: 1294742.2500\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 142316.0312 - val_loss: 1293740.1250\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 142103.7500 - val_loss: 1292711.5000\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 141864.2031 - val_loss: 1291671.1250\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 141647.7188 - val_loss: 1290605.7500\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 141428.8906 - val_loss: 1289544.2500\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 141190.6562 - val_loss: 1288507.5000\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 140974.2656 - val_loss: 1287466.6250\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 140745.9688 - val_loss: 1286451.7500\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 140523.6406 - val_loss: 1285450.3750\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 140309.8594 - val_loss: 1284422.2500\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 140093.7031 - val_loss: 1283393.7500\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 139870.1406 - val_loss: 1282374.6250\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 139656.0625 - val_loss: 1281327.2500\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 139427.6562 - val_loss: 1280294.1250\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 139205.2344 - val_loss: 1279294.2500\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 138987.6719 - val_loss: 1278257.2500\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 138766.6719 - val_loss: 1277213.6250\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 138548.6562 - val_loss: 1276182.8750\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 138326.2656 - val_loss: 1275142.7500\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 138099.4844 - val_loss: 1274091.5000\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 137886.0000 - val_loss: 1273052.6250\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 137661.0625 - val_loss: 1272024.1250\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 137441.5938 - val_loss: 1270973.1250\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 137222.6094 - val_loss: 1269918.1250\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 136992.5000 - val_loss: 1268884.5000\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 136778.1094 - val_loss: 1267855.2500\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 136554.0938 - val_loss: 1266838.2500\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 136337.2344 - val_loss: 1265803.3750\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 136108.7812 - val_loss: 1264757.0000\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 135900.9062 - val_loss: 1263688.0000\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 135674.5469 - val_loss: 1262637.3750\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 135458.0469 - val_loss: 1261612.0000\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 135229.8438 - val_loss: 1260618.6250\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 135028.4844 - val_loss: 1259606.5000\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 134798.8438 - val_loss: 1258618.0000\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 134596.7188 - val_loss: 1257591.0000\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 134381.5938 - val_loss: 1256572.0000\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 134158.4062 - val_loss: 1255561.7500\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 133947.5000 - val_loss: 1254523.6250\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 133737.2031 - val_loss: 1253482.2500\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 133512.5625 - val_loss: 1252455.7500\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 133307.4375 - val_loss: 1251417.8750\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 133086.4844 - val_loss: 1250414.5000\n",
      "Epoch 447/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 8ms/step - loss: 132875.0781 - val_loss: 1249422.5000\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 132674.1250 - val_loss: 1248423.5000\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 132453.9531 - val_loss: 1247434.1250\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 132257.8750 - val_loss: 1246414.3750\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 132048.2500 - val_loss: 1245419.2500\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 131839.6719 - val_loss: 1244440.0000\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 131630.3750 - val_loss: 1243479.5000\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 131421.3594 - val_loss: 1242503.0000\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 131220.7812 - val_loss: 1241496.3750\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 131020.9297 - val_loss: 1240495.1250\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 130802.1250 - val_loss: 1239513.7500\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 130597.4297 - val_loss: 1238535.5000\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 130390.8281 - val_loss: 1237551.0000\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 130185.1641 - val_loss: 1236555.0000\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 129977.0547 - val_loss: 1235531.7500\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 129775.4297 - val_loss: 1234493.0000\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 129555.0391 - val_loss: 1233480.6250\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 129342.6875 - val_loss: 1232490.5000\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 129152.9219 - val_loss: 1231496.2500\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 128940.7266 - val_loss: 1230552.0000\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 128745.7734 - val_loss: 1229622.2500\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 128558.9609 - val_loss: 1228663.0000\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 128367.7266 - val_loss: 1227724.6250\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 128162.7188 - val_loss: 1226811.0000\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 127975.4062 - val_loss: 1225867.5000\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 127778.0938 - val_loss: 1224952.6250\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 127594.3750 - val_loss: 1224108.1250\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 127413.9219 - val_loss: 1223176.7500\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 127218.0000 - val_loss: 1222225.1250\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 127028.5391 - val_loss: 1221342.0000\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 126826.9844 - val_loss: 1220402.2500\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 126658.4219 - val_loss: 1219399.3750\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 126439.6641 - val_loss: 1218427.3750\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 126262.6484 - val_loss: 1217484.7500\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 126054.5781 - val_loss: 1216557.7500\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 125880.2812 - val_loss: 1215585.7500\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 125652.2422 - val_loss: 1214610.3750\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 125469.5703 - val_loss: 1213616.8750\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 125247.6719 - val_loss: 1212613.2500\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 125067.1875 - val_loss: 1211597.2500\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 124833.3750 - val_loss: 1210602.0000\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 124649.2422 - val_loss: 1209570.5000\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 124433.3125 - val_loss: 1208559.0000\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 124229.7031 - val_loss: 1207568.0000\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 124037.9297 - val_loss: 1206581.7500\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 123834.9609 - val_loss: 1205646.7500\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 123645.2969 - val_loss: 1204731.2500\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 123448.9609 - val_loss: 1203811.6250\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 123264.8125 - val_loss: 1202889.5000\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 123072.5547 - val_loss: 1201963.2500\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 122875.2422 - val_loss: 1201000.0000\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 122684.1484 - val_loss: 1200016.1250\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 122492.9844 - val_loss: 1199091.6250\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 122310.3125 - val_loss: 1198178.8750\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using the training data created above using 150 training iterations and a\n",
    "# validation split of 0.05\n",
    "hist = model.fit(rnn_input, rnn_training, batch_size=None, epochs=500, \n",
    "                 verbose=True,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a graph of the training loss/error and the validation loss/error as a function of the\n",
    "# number of training iterations performed.  This is useful to make sure the model is not \n",
    "# overtraining.\n",
    "# Get the data from the trained model and plot it\n",
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(hist.history[label],label=label)\n",
    "# Label the x axis, the y axis, and add a title\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"The final validation loss: {}\".format(hist.history[\"val_loss\"][-1]))\n",
    "# Add a legend then show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn (x_known, y_known):\n",
    "    \"\"\"\n",
    "        Inputs: \n",
    "            x_known (a list or numpy array): the known x data, likely X_tot imported using\n",
    "                the data set.\n",
    "            y_known (a list or numpy array): the known y data, likely y_tot imported using\n",
    "                the data set.\n",
    "        Returns:\n",
    "            None.\n",
    "        Extrapolates from the training data to a complete data set using the trained recurrent\n",
    "        neural network.  Performs data analysis on the predicted data points and creates a graph\n",
    "        of the known data and the predicted data.\n",
    "    \"\"\"\n",
    "    # Segment off the training data from the known data\n",
    "    y_pred = y_known[:dim].tolist()\n",
    "    # Create the first point that will be used to predict the following point using the trained\n",
    "    # recurrent neural network.  In this case the first point contains the two points if the \n",
    "    # training data, so the first point that will be predicted is the first point to fall sequentially\n",
    "    # after the training data\n",
    "    next_input = np.array([[[y_test[dim-2]], [y_test[dim-1]]]])\n",
    "    # Save the last number in the prediction point for later use\n",
    "    last = [y_known[dim-1]]\n",
    "    # Loop until the predicted data set is the same length as the known data set.\n",
    "    for i in range (dim, len(y_known)):\n",
    "        # Predict the next point and add it to the predicted data set\n",
    "        next = model.predict(np.asarray(next_input))\n",
    "        y_pred.append(next[0][0])\n",
    "        # print the difference between the predicted point and the correspinding known point\n",
    "        print ('DIFF: ', next[0][0]-y_known[i])\n",
    "        # Create the point that will be uses to make a prediction on the next interation\n",
    "        next_input = np.array([[last, next[0]]], dtype=np.float64)\n",
    "        last = next\n",
    "    # Print the MSE of the predicted data and the known data.  This is a measure of how well the \n",
    "    # extrapolation worked.\n",
    "    print('MSE: ', np.square(np.subtract(y_known, y_pred)).mean())\n",
    "    # Save the predicted data set as a csv file for future use\n",
    "    name = datatype + 'Predicted'+str(dim)+'.csv'\n",
    "    np.savetxt(name, y_pred, delimiter=',')\n",
    "    # Plot both the known and the predicted data sets and add a legend\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_known, y_known, label=\"true\", linewidth=3)\n",
    "    ax.plot(x_known, y_pred, 'g-.',label=\"predicted\", linewidth=4)\n",
    "    ax.legend()\n",
    "    # Create a semi-transparent red box to represent the training data\n",
    "    ax.axvspan(x_known[0], x_known[dim], alpha=0.25, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1db7a3cfcf60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict the remaining points to finish the data set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-801005f82c54>\u001b[0m in \u001b[0;36mtest_rnn\u001b[0;34m(x_known, y_known)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# training data, so the first point that will be predicted is the first point to fall sequentially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# after the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mnext_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Save the last number in the prediction point for later use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict the remaining points to finish the data set\n",
    "test_rnn(X_tot, y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
