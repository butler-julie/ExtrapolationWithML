{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM LEVEL IMPORTS\n",
    "import sys\n",
    "\n",
    "# THIRD-PARTY IMPORTS\n",
    "# For matrices and calculations\n",
    "import numpy as np\n",
    "# For machine learning (backend for keras)\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_v2_behavior\n",
    "# User-friendly machine learning library\n",
    "# Front end for TensorFlow\n",
    "import keras \n",
    "# Different methods from Keras needed to create an RNN\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "# For graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing the import directory\n",
    "sys.path.append('../DataSets/')\n",
    "# LOCAL IMPORTS\n",
    "# Encoded data sets but can apply this code to any data set\n",
    "from Datesets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a string that represents the name of the data set, a recommended training \n",
    "# dimension for the data, the total x data, and the total y data for a data set\n",
    "# the is encoded in the file DataSets.py\n",
    "# This code can be used with other data sets as long as a training dimension is supplied\n",
    "# with the name \"dim\", the x data is in a one dimensional numpy array named \"X_tot\", and the\n",
    "# y data is in a one dimensional numpy array called \"y_tot\".\n",
    "name, dim, X_tot, y_tot = VaryDimension()\n",
    "# Check to see if the data set is complete\n",
    "assert len(X_tot) == len(y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data, length_of_sequence = 2):  \n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            data(a numpy array): the data that will be the inputs to the recurrent neural\n",
    "                network\n",
    "            length_of_sequence (an int): the number of elements in one iteration of the\n",
    "                sequence patter.  For a function approximator use length_of_sequence = 2.\n",
    "        Returns:\n",
    "            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its\n",
    "                dimensions are length of data - length of sequence, length of sequence, \n",
    "                dimnsion of data\n",
    "            rnn_output (a numpy array): the training data for the neural network\n",
    "        Formats data to be used in a recurrent neural network.  The resulting data points have the\n",
    "        following format for a sequence length of n: ((y1, y2, ..., yn), yn+1).  This function is \n",
    "        adapted from the one found here: \n",
    "        https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "    \"\"\"\n",
    "    # To store the formated \"x\" and \"y\" data\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data)-length_of_sequence):\n",
    "        # Get the next length_of_sequence elements\n",
    "        a = data[i:i+length_of_sequence]\n",
    "        # Get the element that immediately follows that\n",
    "        b = data[i+length_of_sequence]\n",
    "        # Reshape so that each data point is contained in its own array\n",
    "        a = np.reshape (a, (len(a), 1))\n",
    "        X.append(a)\n",
    "        Y.append(b)\n",
    "    # Convert into numpy arrays as these are easier to use later in the code.\n",
    "    rnn_input = np.array(X)\n",
    "    rnn_output = np.array(Y)\n",
    "    return rnn_input, rnn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(length_of_sequences, batch_size = None, stateful = False):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            length_of_sequence (an int): the length of sequence used to format the training \n",
    "                data (i.e. the length of the sequence used in format_data).\n",
    "            batch_size (an int): See Keras documentation for Input\n",
    "            stateful (a boolean): See Keras documentation for SimpleRNN\n",
    "        Returns:\n",
    "            model (a Keras model): the build and compiled recurrent neural network\n",
    "        Creates a simple recurrent neural network with one simple recurrent hidden layer with\n",
    "        200 hidden neurons and compiles the network using a mean-squared error loss function \n",
    "        and an Adam's optimizer.\n",
    "    \"\"\"\n",
    "    # Number of neurons in the input and output layer\n",
    "    in_out_neurons = 1\n",
    "    # Number of neurons in the hidden layer\n",
    "    hidden_neurons = 200\n",
    "    # Create the input layer\n",
    "    inp = Input(batch_shape=(batch_size, \n",
    "                length_of_sequences, \n",
    "                in_out_neurons))  \n",
    "    # Create the simple recurrent hidden layer\n",
    "    rnn = SimpleRNN(hidden_neurons, \n",
    "                    return_sequences=False,\n",
    "                    stateful = stateful,\n",
    "                    name=\"RNN\")(inp)\n",
    "    # Create a dense (feedforward) neural network layer which will act as the output layer\n",
    "    dens = Dense(in_out_neurons,name=\"dense\")(rnn)\n",
    "    # Build the model\n",
    "    model = Model(inputs=[inp],outputs=[dens])\n",
    "    # Compile the model using the specified loss function and optimizer\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data for the RNN using a sequence length of 2\n",
    "# Get the first dim points from the total data set to use for training\n",
    "X_train = X_tot[:dim]\n",
    "y_train = y_tot[:dim]\n",
    "# Formating the y component of the training data using the time series forecasting \n",
    "rnn_input, rnn_training = format_data(y_train, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the default values for batch_size, stateful\n",
    "model = rnn(length_of_sequences = rnn_input.shape[1])\n",
    "# Print a summary of the Keras model to the console\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the training data created above using 150 training iterations and a\n",
    "# validation split of 0.05\n",
    "hist = model.fit(rnn_input, rnn_training, batch_size=None, epochs=150, \n",
    "                 verbose=True,validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a graph of the training loss/error and the validation loss/error as a function of the\n",
    "# number of training iterations performed.  This is useful to make sure the model is not \n",
    "# overtraining.\n",
    "# Get the data from the trained model and plot it\n",
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(hist.history[label],label=label)\n",
    "# Label the x axis, the y axis, and add a title\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"The final validation loss: {}\".format(hist.history[\"val_loss\"][-1]))\n",
    "# Add a legend then show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn (x_known, y_known):\n",
    "    \"\"\"\n",
    "        Inputs: \n",
    "            x_known (a list or numpy array): the known x data, likely X_tot imported using\n",
    "                the data set.\n",
    "            y_known (a list or numpy array): the known y data, likely y_tot imported using\n",
    "                the data set.\n",
    "        Returns:\n",
    "            None.\n",
    "        Extrapolates from the training data to a complete data set using the trained recurrent\n",
    "        neural network.  Performs data analysis on the predicted data points and creates a graph\n",
    "        of the known data and the predicted data.\n",
    "    \"\"\"\n",
    "    # Segment off the training data from the known data\n",
    "    y_pred = y_known[:dim].tolist()\n",
    "    # Create the first point that will be used to predict the following point using the trained\n",
    "    # recurrent neural network.  In this case the first point contains the two points if the \n",
    "    # training data, so the first point that will be predicted is the first point to fall sequentially\n",
    "    # after the training data\n",
    "    next_input = np.array([[[y_test[dim-2]], [y_test[dim-1]]]])\n",
    "    # Save the last number in the prediction point for later use\n",
    "    last = [y_test[dim-1]]\n",
    "    # Loop until the predicted data set is the same length as the known data set.\n",
    "    for i in range (dim, len(y_known)):\n",
    "        # Predict the next point and add it to the predicted data set\n",
    "        next = model.predict(np.asarray(next_input))\n",
    "        y_pred.append(next[0][0])\n",
    "        # print the difference between the predicted point and the correspinding known point\n",
    "        print ('DIFF: ', next[0][0]-y_known[i])\n",
    "        # Create the point that will be uses to make a prediction on the next interation\n",
    "        next_input = np.array([[last, next[0]]], dtype=np.float64)\n",
    "        last = next\n",
    "    # Print the MSE of the predicted data and the known data.  This is a measure of how well the \n",
    "    # extrapolation worked.\n",
    "    print('MSE: ', np.square(np.subtract(y_known, y_pred)).mean())\n",
    "    # Save the predicted data set as a csv file for future use\n",
    "    name = datatype + 'Predicted'+str(dim)+'.csv'\n",
    "    np.savetxt(name, y_pred, delimiter=',')\n",
    "    # Plot both the known and the predicted data sets and add a legend\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_known, y_known, label=\"true\", linewidth=3)\n",
    "    ax.plot(x_known, y_pred, 'g-.',label=\"predicted\", linewidth=4)\n",
    "    ax.legend()\n",
    "    # Create a semi-transparent red box to represent the training data\n",
    "    ax.axvspan(x_known[0], x_known[dim], alpha=0.25, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the remaining points to finish the data set\n",
    "test_rnn(X_tot, y_tot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
