{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM LEVEL IMPORTS\n",
    "import sys\n",
    "\n",
    "# THIRD-PARTY IMPORTS\n",
    "# For matrices and calculations\n",
    "import numpy as np\n",
    "# For machine learning (backend for keras)\n",
    "import tensorflow as tf\n",
    "#tf.compat.v1.disable_v2_behavior\n",
    "# User-friendly machine learning library\n",
    "# Front end for TensorFlow\n",
    "import keras \n",
    "# Different methods from Keras needed to create an RNN\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.layers.recurrent import SimpleRNN, GRU\n",
    "# For graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing the import directory\n",
    "sys.path.append('../DataSets/')\n",
    "# LOCAL IMPORTS\n",
    "# Encoded data sets but can apply this code to any data set\n",
    "#from Datasets import *\n",
    "from EquationOfState import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a string that represents the name of the data set, a recommended training \n",
    "# dimension for the data, the total x data, and the total y data for a data set\n",
    "# the is encoded in the file DataSets.py\n",
    "# This code can be used with other data sets as long as a training dimension is supplied\n",
    "# with the name \"dim\", the x data is in a one dimensional numpy array named \"X_tot\", and the\n",
    "# y data is in a one dimensional numpy array called \"y_tot\".\n",
    "X_tot, y_tot, design_matrix = EquationOfState()\n",
    "dim = 80\n",
    "# Check to see if the data set is complete\n",
    "assert len(X_tot) == len(y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data, length_of_sequence = 2):  \n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            data(a numpy array): the data that will be the inputs to the recurrent neural\n",
    "                network\n",
    "            length_of_sequence (an int): the number of elements in one iteration of the\n",
    "                sequence patter.  For a function approximator use length_of_sequence = 2.\n",
    "        Returns:\n",
    "            rnn_input (a 3D numpy array): the input data for the recurrent neural network.  Its\n",
    "                dimensions are length of data - length of sequence, length of sequence, \n",
    "                dimnsion of data\n",
    "            rnn_output (a numpy array): the training data for the neural network\n",
    "        Formats data to be used in a recurrent neural network.  The resulting data points have the\n",
    "        following format for a sequence length of n: ((y1, y2, ..., yn), yn+1).  This function is \n",
    "        adapted from the one found here: \n",
    "        https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "    \"\"\"\n",
    "    # To store the formated \"x\" and \"y\" data\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data)-length_of_sequence):\n",
    "        # Get the next length_of_sequence elements\n",
    "        a = data[i:i+length_of_sequence]\n",
    "        # Get the element that immediately follows that\n",
    "        b = data[i+length_of_sequence]\n",
    "        # Reshape so that each data point is contained in its own array\n",
    "        a = np.reshape (a, (len(a), 1))\n",
    "        X.append(a)\n",
    "        Y.append(b)\n",
    "    # Convert into numpy arrays as these are easier to use later in the code.\n",
    "    rnn_input = np.array(X)\n",
    "    rnn_output = np.array(Y)\n",
    "    return rnn_input, rnn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(length_of_sequences, hidden_neurons, loss, optimizer, activation_dense, activation_rnn, rate, batch_size = None, stateful = False):\n",
    "    \"\"\"\n",
    "        Inputs:\n",
    "            length_of_sequences (an int): the length of sequence used to format the data set\n",
    "            hidden_neurons (an int): the number of neurons to be used in the SimpleRNN layers, or double the number\n",
    "                of neurons to be used in the Dense layers\n",
    "            loss (a string): the loss function to be used\n",
    "            optimizer (a string): the optimizer to be used\n",
    "            activation (a string): the activation to be used in the dense layers\n",
    "            rate (an int or float): the L2 regulization rate (not used in this example)\n",
    "            batch_size (an int): Default value is None.  See Keras documentation of SimpleRNN.\n",
    "            stateful (a boolean): Default value is False.  See Keras documentation of SimpleRNN.\n",
    "        Returns:\n",
    "            model (a Keras model): a compiled recurrent neural network consisting of one input layer followed by\n",
    "                2 dense (feedforward layers), then three simple recurrent neural network layers, and finally an\n",
    "                output layer.\n",
    "        Builds and compiles a Keras recurrent neural network with specified parameters using two hidden dense \n",
    "        layers followed by three hidden simple recurrent neural network layers.\n",
    "    \"\"\"\n",
    "    # Number of neurons in the input and output layers\n",
    "    in_out_neurons = 1\n",
    "    # Input layer\n",
    "    inp = Input(batch_shape=(batch_size, \n",
    "                length_of_sequences, \n",
    "                in_out_neurons)) \n",
    "    # Hidden dense layers\n",
    "    dnn = Dense(hidden_neurons/2, activation=activation_dense, name='dnn')(inp)\n",
    "    dnn1 = Dense(hidden_neurons/2, activation=activation_dense, name='dnn1')(dnn)\n",
    "    # Hidden simple recurrent layers\n",
    "    rnn1 = GRU(hidden_neurons, \n",
    "                    return_sequences=True, activation = activation_rnn,\n",
    "                    stateful = stateful,\n",
    "                    name=\"RNN1\", use_bias=True,recurrent_dropout=0.0, kernel_regularizer=keras.regularizers.l2(rate))(dnn1)\n",
    "    rnn2 = GRU(hidden_neurons, \n",
    "                    return_sequences=True, activation = activation_rnn,\n",
    "                    stateful = stateful,\n",
    "                    name=\"RNN2\", use_bias=True,recurrent_dropout=0.0, kernel_regularizer=keras.regularizers.l2(rate))(rnn1)\n",
    "    rnn = GRU(hidden_neurons, \n",
    "                    return_sequences=False, activation = activation_rnn,\n",
    "                    stateful = stateful,\n",
    "                    name=\"RNN\", use_bias=True,recurrent_dropout=0.0, kernel_regularizer=keras.regularizers.l2(rate))(rnn2)\n",
    "    # Output layer\n",
    "    dens = Dense(in_out_neurons,name=\"dense\")(rnn)\n",
    "    # Build the model\n",
    "    model = Model(inputs=[inp],outputs=[dens])\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss, optimizer=optimizer)  \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data for the RNN using a sequence length of 2\n",
    "# Get the first dim points from the total data set to use for training\n",
    "X_train = X_tot[:dim]\n",
    "y_train = y_tot[:dim]\n",
    "\n",
    "X_train = X_train.reshape((-1, 1))\n",
    "# Formating the y component of the training data using the time series forecasting \n",
    "rnn_input, rnn_training = format_data(y_train, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1, 1)]            0         \n",
      "_________________________________________________________________\n",
      "dnn (Dense)                  (None, 1, 125)            250       \n",
      "_________________________________________________________________\n",
      "dnn1 (Dense)                 (None, 1, 125)            15750     \n",
      "_________________________________________________________________\n",
      "RNN1 (GRU)                   (None, 1, 250)            282750    \n",
      "_________________________________________________________________\n",
      "RNN2 (GRU)                   (None, 1, 250)            376500    \n",
      "_________________________________________________________________\n",
      "RNN (GRU)                    (None, 250)               376500    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 1,052,001\n",
      "Trainable params: 1,052,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## use the default values for batch_size, stateful\n",
    "model = rnn(1, 250, 'mse', 'adam', 'relu', 'relu', 0.0, batch_size = None, stateful = False)\n",
    "# Print a summary of the Keras model to the console\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 285374.6250 - val_loss: 1698771.0000\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 285346.6562 - val_loss: 1698594.7500\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 285295.1250 - val_loss: 1698233.7500\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 285206.7188 - val_loss: 1697517.3750\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 285005.6250 - val_loss: 1696023.3750\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 284638.7812 - val_loss: 1692717.0000\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 283830.0000 - val_loss: 1685032.2500\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 282218.0938 - val_loss: 1666430.2500\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 278576.4688 - val_loss: 1624554.7500\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 269717.5312 - val_loss: 1548050.7500\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 254327.3906 - val_loss: 1422511.7500\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 225387.0938 - val_loss: 1226347.1250\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 182900.0625 - val_loss: 946144.8750\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 124853.9609 - val_loss: 580782.1250\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 57736.2734 - val_loss: 199342.9375\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 18461.5078 - val_loss: 9781.9434\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 42660.5664 - val_loss: 2872.8213\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 42390.7383 - val_loss: 30797.4121\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 20414.9062 - val_loss: 121275.9141\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 16713.2031 - val_loss: 191366.7812\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 19837.8965 - val_loss: 200236.5312\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 19469.7734 - val_loss: 160493.7188\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 15280.9912 - val_loss: 104181.2578\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 11847.6035 - val_loss: 59373.1602\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11218.3438 - val_loss: 38121.6250\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 11248.2930 - val_loss: 37332.8320\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 9876.0879 - val_loss: 44934.2148\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 8334.1162 - val_loss: 60014.5391\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 7484.2603 - val_loss: 66778.9219\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 7003.1768 - val_loss: 62986.9180\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 6209.6309 - val_loss: 52680.1016\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5232.0479 - val_loss: 37149.4766\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4376.5635 - val_loss: 26286.3145\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3869.2717 - val_loss: 19268.5020\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3409.0942 - val_loss: 17308.1875\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2723.9553 - val_loss: 20176.9141\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 2099.1816 - val_loss: 22561.8672\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1785.6311 - val_loss: 20397.6426\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1471.5081 - val_loss: 13600.8115\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1084.8315 - val_loss: 8289.0293\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 889.3745 - val_loss: 6187.0181\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 730.0485 - val_loss: 5829.2539\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 611.2943 - val_loss: 5635.9253\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 566.7877 - val_loss: 4043.3896\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 504.6949 - val_loss: 2437.1475\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 479.1571 - val_loss: 1987.1138\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 459.4064 - val_loss: 2123.4321\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 448.8186 - val_loss: 1980.0491\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 438.0062 - val_loss: 1654.4233\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 427.8208 - val_loss: 1517.4666\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 422.7274 - val_loss: 1616.3744\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 411.6246 - val_loss: 1807.4312\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 405.7024 - val_loss: 1895.6458\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 396.8401 - val_loss: 1906.1428\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 385.8298 - val_loss: 1843.6486\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 377.3723 - val_loss: 2005.7124\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 367.9439 - val_loss: 2207.9834\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 359.4267 - val_loss: 2254.8918\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 351.3527 - val_loss: 2238.7700\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 347.7514 - val_loss: 2421.3599\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 338.1468 - val_loss: 2833.3567\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 333.7072 - val_loss: 3154.6045\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 328.9318 - val_loss: 3108.8945\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 320.9983 - val_loss: 2914.3862\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 313.3994 - val_loss: 2903.8994\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 307.6038 - val_loss: 3121.3267\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 346.588 - 0s 17ms/step - loss: 302.4438 - val_loss: 3236.5457\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 296.9871 - val_loss: 3164.5239\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 290.3948 - val_loss: 3078.9702\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 284.3537 - val_loss: 3011.3999\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 279.1591 - val_loss: 2944.4700\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 273.6292 - val_loss: 2778.7358\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 269.6837 - val_loss: 2887.6211\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 264.1212 - val_loss: 3092.9663\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 258.0389 - val_loss: 3083.1248\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 254.2597 - val_loss: 3061.4741\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 248.8588 - val_loss: 3166.4023\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 14ms/step - loss: 245.4752 - val_loss: 3316.3235\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 240.5061 - val_loss: 3074.5308\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 235.0336 - val_loss: 2994.1060\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 230.6757 - val_loss: 3149.3521\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 226.0617 - val_loss: 3232.9951\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 221.8405 - val_loss: 3228.2266\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 217.5653 - val_loss: 3116.8328\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 212.7410 - val_loss: 3122.9417\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 207.9378 - val_loss: 2969.7229\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 204.9267 - val_loss: 2929.1318\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 201.0015 - val_loss: 2979.6018\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 196.0931 - val_loss: 3274.8760\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 195.0292 - val_loss: 3339.6997\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 189.8583 - val_loss: 3009.3521\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 186.1481 - val_loss: 2854.5027\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 182.9729 - val_loss: 2947.4346\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 179.1216 - val_loss: 2820.2791\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 174.7780 - val_loss: 3048.1843\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 170.9818 - val_loss: 3288.4067\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 169.3042 - val_loss: 3173.4150\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 165.3381 - val_loss: 2810.2112\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 161.6478 - val_loss: 2831.8799\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 157.5407 - val_loss: 3169.1946\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 155.6886 - val_loss: 3214.9712\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 153.0762 - val_loss: 3102.1934\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 148.5678 - val_loss: 2972.3403\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 145.3533 - val_loss: 2862.7280\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 142.5211 - val_loss: 2945.1943\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 139.8824 - val_loss: 3189.9541\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 137.7405 - val_loss: 3162.3220\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 134.2552 - val_loss: 2869.6729\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 133.0680 - val_loss: 2525.3364\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 134.1323 - val_loss: 2831.0381\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 128.8158 - val_loss: 3114.4819\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 125.2841 - val_loss: 2884.4829\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 122.4912 - val_loss: 2871.9939\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 120.5469 - val_loss: 2865.0977\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 118.9504 - val_loss: 2974.1067\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 116.8445 - val_loss: 2931.8843\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 114.6261 - val_loss: 2739.3040\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 114.4595 - val_loss: 2698.0854\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 112.9828 - val_loss: 3018.1958\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 111.0188 - val_loss: 3191.9797\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 110.6699 - val_loss: 3088.7793\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 108.1311 - val_loss: 2996.1721\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 106.0633 - val_loss: 2950.5200\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 104.6926 - val_loss: 2915.8809\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 103.1577 - val_loss: 2955.4136\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 101.8852 - val_loss: 2799.0884\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 101.0555 - val_loss: 2600.9854\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 100.6943 - val_loss: 2759.2483\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 98.6091 - val_loss: 2952.6260\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 100.5143 - val_loss: 2948.2026\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 96.4137 - val_loss: 2421.9050\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 100.4489 - val_loss: 2440.2197\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 93.8765 - val_loss: 3103.0493\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 100.3855 - val_loss: 3290.0967\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 100.7489 - val_loss: 2830.5703\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 91.8065 - val_loss: 2619.3950\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 90.5855 - val_loss: 2483.4609\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 91.0749 - val_loss: 2395.9702\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 89.0996 - val_loss: 2744.4678\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 89.0863 - val_loss: 3045.2500\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 92.2756 - val_loss: 2738.7678\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 86.6255 - val_loss: 2343.5862\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 86.7275 - val_loss: 2247.5535\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 86.6480 - val_loss: 2480.1611\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 84.2682 - val_loss: 2700.7241\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 84.7800 - val_loss: 2543.6606\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 82.6499 - val_loss: 2565.2373\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 82.2683 - val_loss: 2510.2993\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 81.3536 - val_loss: 2255.8259\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 81.0152 - val_loss: 2407.6760\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 78.8302 - val_loss: 2711.2849\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 81.8128 - val_loss: 2717.4380\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 79.6824 - val_loss: 2300.5015\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 78.7755 - val_loss: 1974.4878\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 82.5335 - val_loss: 1974.2097\n",
      "Epoch 156/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 13ms/step - loss: 81.2028 - val_loss: 2162.8438\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 75.0135 - val_loss: 2657.1042\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 78.3822 - val_loss: 2442.7766\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 75.3248 - val_loss: 2131.1919\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 74.0816 - val_loss: 2189.6716\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 74.0768 - val_loss: 2346.3442\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 73.1484 - val_loss: 2084.0874\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 72.0015 - val_loss: 2000.9417\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 71.4276 - val_loss: 2217.2085\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 70.5149 - val_loss: 2476.4360\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 72.7986 - val_loss: 2175.0188\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 70.0832 - val_loss: 1908.0598\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 69.1560 - val_loss: 2255.9980\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 69.1863 - val_loss: 2415.5371\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 70.0520 - val_loss: 2136.5591\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 68.0757 - val_loss: 1837.4626\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 69.1529 - val_loss: 1975.4601\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 66.3685 - val_loss: 2181.5688\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 66.1023 - val_loss: 1946.5675\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 65.8689 - val_loss: 1773.7705\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 65.2820 - val_loss: 2103.2051\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 65.4654 - val_loss: 2128.0249\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 65.3098 - val_loss: 1707.3420\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 64.0309 - val_loss: 1929.2833\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 62.9785 - val_loss: 2087.9187\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 62.6607 - val_loss: 1695.3466\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 64.0251 - val_loss: 1583.8110\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 62.3930 - val_loss: 1997.8508\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 61.8730 - val_loss: 2041.3390\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 60.9750 - val_loss: 1782.8909\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 62.2489 - val_loss: 1778.4265\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 60.4854 - val_loss: 2042.8915\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 59.8026 - val_loss: 1733.8419\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 60.2103 - val_loss: 1568.2449\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 59.8883 - val_loss: 1721.7615\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 57.9475 - val_loss: 1751.6963\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 57.8361 - val_loss: 1827.2183\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 57.3206 - val_loss: 2025.7177\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 59.7285 - val_loss: 1900.8347\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 56.9816 - val_loss: 1564.8529\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 57.3528 - val_loss: 1421.6796\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 57.8937 - val_loss: 1741.9768\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 55.1690 - val_loss: 1942.9456\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 56.7515 - val_loss: 1640.2739\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 55.2223 - val_loss: 1392.6079\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 55.7285 - val_loss: 1788.0239\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 55.4440 - val_loss: 1834.4812\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 53.2397 - val_loss: 1414.7019\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 54.6147 - val_loss: 1577.3473\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 52.1358 - val_loss: 1851.0540\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 54.6237 - val_loss: 1508.1406\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 56.7633 - val_loss: 1173.0817\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 56.4519 - val_loss: 1829.2340\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 54.1258 - val_loss: 1771.6500\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 51.0899 - val_loss: 1415.9148\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 52.1839 - val_loss: 1461.8398\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 49.9395 - val_loss: 1952.9736\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 52.4295 - val_loss: 1599.3760\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 50.3157 - val_loss: 1373.5178\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 52.0014 - val_loss: 1624.9768\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 48.9279 - val_loss: 1564.9697\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 48.7893 - val_loss: 1745.1584\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 48.9580 - val_loss: 1815.2939\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 48.7989 - val_loss: 1553.2375\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 47.6010 - val_loss: 1546.6860\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 47.0806 - val_loss: 1771.2422\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 47.7576 - val_loss: 1710.7600\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 46.8605 - val_loss: 1451.7023\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 46.1806 - val_loss: 1427.8984\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 46.3404 - val_loss: 1512.1890\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 45.0257 - val_loss: 1335.0789\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 45.9395 - val_loss: 1490.9824\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 45.9696 - val_loss: 1778.2786\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 47.7126 - val_loss: 1361.9299\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 45.5489 - val_loss: 1192.4248\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 45.3155 - val_loss: 1621.2946\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 45.1015 - val_loss: 1575.9314\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 43.0707 - val_loss: 1312.0153\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 44.4536 - val_loss: 1407.5532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 42.1693 - val_loss: 1622.4058\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 42.2576 - val_loss: 1554.7556\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 41.6937 - val_loss: 1545.3838\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 40.29 - 0s 12ms/step - loss: 41.5120 - val_loss: 1534.5592\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 40.9857 - val_loss: 1594.6417\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 40.8363 - val_loss: 1458.1256\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 40.6666 - val_loss: 1468.2853\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 40.8787 - val_loss: 1380.6438\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 39.8694 - val_loss: 1547.6726\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 40.4769 - val_loss: 1491.9298\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 39.7813 - val_loss: 1386.6339\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 38.7051 - val_loss: 1585.0481\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 39.7315 - val_loss: 1346.5115\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 39.8997 - val_loss: 1309.9304\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 37.6283 - val_loss: 1768.9088\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 44.4627 - val_loss: 1426.8572\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 39.3394 - val_loss: 1031.6628\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 39.9468 - val_loss: 1584.0303\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 40.4465 - val_loss: 1601.4534\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 38.5447 - val_loss: 1166.4883\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 37.7731 - val_loss: 1260.9652\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 36.1043 - val_loss: 1544.5229\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 37.7832 - val_loss: 1453.6107\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 35.6110 - val_loss: 1188.3845\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 36.4865 - val_loss: 1200.8251\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 35.5347 - val_loss: 1497.8680\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 35.8159 - val_loss: 1406.7433\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 35.2010 - val_loss: 1085.4374\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 36.1866 - val_loss: 1454.1516\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 34.5069 - val_loss: 1477.2816\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 34.0903 - val_loss: 1194.2699\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 34.3826 - val_loss: 1083.6603\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 35.2457 - val_loss: 1320.6990\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 32.9819 - val_loss: 1445.7683\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 33.2886 - val_loss: 1382.6033\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 33.1574 - val_loss: 1338.4033\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 32.1294 - val_loss: 1427.4561\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 32.5483 - val_loss: 1382.4185\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 32.4132 - val_loss: 1298.3918\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 33.2224 - val_loss: 1018.3674\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 33.9542 - val_loss: 1484.3958\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 32.8192 - val_loss: 1595.6223\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 32.3784 - val_loss: 1144.5049\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 33.2071 - val_loss: 1092.4258\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 32.9645 - val_loss: 1562.6526\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 31.8566 - val_loss: 1330.3748\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 29.8645 - val_loss: 1165.1709\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 30.4797 - val_loss: 1234.6069\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 29.6478 - val_loss: 1321.0231\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 29.3372 - val_loss: 1263.1708\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 28.9557 - val_loss: 1311.3723\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 29.2652 - val_loss: 1130.1196\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 30.2759 - val_loss: 1020.4894\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 29.9870 - val_loss: 1451.4458\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 29.8658 - val_loss: 1300.1628\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 28.8757 - val_loss: 1142.0004\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 28.4507 - val_loss: 1482.4779\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 28.3540 - val_loss: 1232.9020\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 27.2629 - val_loss: 1276.3043\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 28.0491 - val_loss: 1228.2903\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 28.3202 - val_loss: 1155.9854\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 26.7248 - val_loss: 1607.1693\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 29.7625 - val_loss: 1112.0266\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 30.8909 - val_loss: 965.1019\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 27.6944 - val_loss: 1829.6509\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 37.0895 - val_loss: 1147.8240\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 29.7279 - val_loss: 826.7562\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 30.0253 - val_loss: 1568.4106\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 31.4468 - val_loss: 1379.1204\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 25.9392 - val_loss: 978.1169\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 26.6619 - val_loss: 1357.8972\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 26.9156 - val_loss: 1414.9688\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 25.3388 - val_loss: 1039.2930\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 26.9615 - val_loss: 1281.6047\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 24.7923 - val_loss: 1558.0059\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 26.1687 - val_loss: 1115.1239\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 25.6753 - val_loss: 1050.7955\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 24.5377 - val_loss: 1421.7075\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 14ms/step - loss: 24.7329 - val_loss: 1392.9182\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 23.8786 - val_loss: 1159.2878\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 23.2073 - val_loss: 1221.7754\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 22.6760 - val_loss: 1311.7224\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 22.8693 - val_loss: 1158.3784\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 22.4833 - val_loss: 1100.8230\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 22.6082 - val_loss: 1171.8398\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 21.9607 - val_loss: 1308.2411\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 22.3334 - val_loss: 1178.4900\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 21.6422 - val_loss: 1142.2397\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 21.5946 - val_loss: 1231.3695\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 21.3620 - val_loss: 1129.7219\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 21.5195 - val_loss: 944.7706\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 24.8538 - val_loss: 1001.0585\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 21.8959 - val_loss: 1368.4934\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 21.7479 - val_loss: 1042.2662\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 21.0762 - val_loss: 1120.7615\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 20.0928 - val_loss: 1230.7546\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 20.3127 - val_loss: 1150.6040\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 19.7825 - val_loss: 1154.3445\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 19.6930 - val_loss: 1244.3269\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 20.0580 - val_loss: 1076.9915\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 19.8157 - val_loss: 1043.7955\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 19.4208 - val_loss: 1407.1273\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 21.5216 - val_loss: 1144.3872\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 18.6699 - val_loss: 946.7743\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 20.4917 - val_loss: 1205.7860\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 18.8866 - val_loss: 1142.0658\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 18.7613 - val_loss: 936.8549\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 19.1836 - val_loss: 1217.2886\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 19.8169 - val_loss: 1201.8879\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 18.3720 - val_loss: 982.9117\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 18.0644 - val_loss: 1156.1085\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 18.4869 - val_loss: 1010.4734\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 19.7513 - val_loss: 935.6426\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 18.9179 - val_loss: 1369.2577\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 19.3693 - val_loss: 884.9497\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 20.8588 - val_loss: 1017.5931\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 18.7246 - val_loss: 1430.7567\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 21.2176 - val_loss: 914.5449\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 17.9430 - val_loss: 994.5040\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 16.4527 - val_loss: 1179.8315\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 17.3550 - val_loss: 1128.2258\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 16.5514 - val_loss: 1002.3980\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 16.2137 - val_loss: 1124.6266\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 16.6251 - val_loss: 1128.8990\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 16.1498 - val_loss: 962.1302\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 16.0727 - val_loss: 1104.0002\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 15.7150 - val_loss: 1091.4576\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 15.2615 - val_loss: 1068.1353\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 15.4233 - val_loss: 1124.4736\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 15.1269 - val_loss: 1051.5737\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 15.1794 - val_loss: 985.8584\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 15.3706 - val_loss: 1045.8746\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 14.7188 - val_loss: 1053.5094\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 14.5765 - val_loss: 1025.3501\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.4914 - val_loss: 1025.9716\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.4314 - val_loss: 1030.8829\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 14.7488 - val_loss: 1002.8015\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 14.0721 - val_loss: 1132.4983\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 14.0107 - val_loss: 875.3530\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 15.8372 - val_loss: 872.0159\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 14.3215 - val_loss: 1165.6956\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 15.0877 - val_loss: 1037.0725\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 13.2355 - val_loss: 860.3143\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 14.0961 - val_loss: 1062.2358\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 13.2066 - val_loss: 1055.1108\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.9676 - val_loss: 1005.5651\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.5359 - val_loss: 1013.9688\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.3716 - val_loss: 944.3763\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.2889 - val_loss: 924.0452\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 12.0354 - val_loss: 1112.9438\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 12.7139 - val_loss: 1045.8669\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 11.2143 - val_loss: 839.2508\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.9254 - val_loss: 1043.0884\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 12.6674 - val_loss: 1087.6960\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 11.3868 - val_loss: 795.1064\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 13.2836 - val_loss: 1043.9242\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 10.9596 - val_loss: 988.2716\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 13ms/step - loss: 10.7470 - val_loss: 955.3499\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 10.5332 - val_loss: 1065.3225\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 10.1030 - val_loss: 922.7482\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.9689 - val_loss: 909.7311\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 9.7304 - val_loss: 985.5202\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.2849 - val_loss: 959.2942\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.1375 - val_loss: 996.9493\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 9.3723 - val_loss: 993.9833\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 8.8755 - val_loss: 803.6996\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 10.7547 - val_loss: 926.5072\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 8.8586 - val_loss: 1017.2446\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 8.7445 - val_loss: 762.9655\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.5151 - val_loss: 1069.4846\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 9.5183 - val_loss: 993.9744\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 8.0027 - val_loss: 837.9435\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 7.9979 - val_loss: 1063.1537\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 8.1191 - val_loss: 957.7664\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 7.2482 - val_loss: 868.2630\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.7099 - val_loss: 975.4425\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 7.1826 - val_loss: 970.0774\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.0644 - val_loss: 873.7833\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 7.3203 - val_loss: 923.1877\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.5270 - val_loss: 860.0054\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.9801 - val_loss: 1039.9451\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.1231 - val_loss: 895.7039\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 7.9264 - val_loss: 843.1295\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.3950 - val_loss: 1014.2432\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.3695 - val_loss: 859.7377\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.5305 - val_loss: 924.8734\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 5.8411 - val_loss: 997.9029\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.1204 - val_loss: 814.7270\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 7.8704 - val_loss: 896.5515\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 6.5477 - val_loss: 1253.9154\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 9.3643 - val_loss: 789.7452\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 8.1027 - val_loss: 880.5331\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.1836 - val_loss: 1009.0367\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.7437 - val_loss: 786.9999\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 6.2215 - val_loss: 1047.3019\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.9803 - val_loss: 942.3096\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.9775 - val_loss: 928.0795\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.4265 - val_loss: 996.7443\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 5.2360 - val_loss: 932.3375\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.0360 - val_loss: 961.7378\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.1897 - val_loss: 960.6593\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.3314 - val_loss: 974.8303\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.7610 - val_loss: 829.0980\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 4.9850 - val_loss: 933.0811\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 5.1651 - val_loss: 953.2659\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.8143 - val_loss: 849.7053\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.6700 - val_loss: 943.8022\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.7232 - val_loss: 816.5452\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 5.6427 - val_loss: 892.0251\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.1269 - val_loss: 1040.3062\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.7883 - val_loss: 771.8082\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.0236 - val_loss: 1005.4111\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 5.0361 - val_loss: 957.6289\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.9522 - val_loss: 785.1515\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 5.3479 - val_loss: 919.9952\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.1122 - val_loss: 1062.8879\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.0610 - val_loss: 747.5905\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.8190 - val_loss: 817.7735\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.2036 - val_loss: 1047.5425\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.9844 - val_loss: 779.2699\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.4825 - val_loss: 956.4450\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.4190 - val_loss: 968.9226\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.0183 - val_loss: 763.0875\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.8308 - val_loss: 844.9327\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.8755 - val_loss: 797.4377\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.2999 - val_loss: 852.0154\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.3884 - val_loss: 842.3483\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.5995 - val_loss: 717.4249\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.1824 - val_loss: 1074.9817\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.2548 - val_loss: 845.8245\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.6300 - val_loss: 872.0134\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.9029 - val_loss: 932.9971\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.5711 - val_loss: 735.9592\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.9127 - val_loss: 892.4625\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 4.9222 - val_loss: 986.4795\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.3305 - val_loss: 794.8011\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.4947 - val_loss: 972.1594\n",
      "Epoch 472/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 20ms/step - loss: 4.1107 - val_loss: 861.4823\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.3914 - val_loss: 838.4366\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.3159 - val_loss: 895.3379\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.7450 - val_loss: 820.1382\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.7301 - val_loss: 843.7783\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.5247 - val_loss: 881.7654\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.6674 - val_loss: 748.5276\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.6481 - val_loss: 967.6965\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.2586 - val_loss: 802.6691\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.3032 - val_loss: 826.8331\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.0480 - val_loss: 933.2647\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.7405 - val_loss: 762.4883\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.2608 - val_loss: 874.4406\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.8417 - val_loss: 847.9686\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.2933 - val_loss: 773.7719\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.2793 - val_loss: 860.1563\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.5618 - val_loss: 831.3468\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.1169 - val_loss: 800.0836\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.0740 - val_loss: 938.9146\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.0124 - val_loss: 815.9678\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.3654 - val_loss: 753.5233\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.0335 - val_loss: 933.5604\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.1022 - val_loss: 839.4845\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.9611 - val_loss: 736.1946\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.4879 - val_loss: 812.1586\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 3.0647 - val_loss: 770.3234\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.6581 - val_loss: 758.4274\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.1550 - val_loss: 920.6904\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.6285 - val_loss: 764.8144\n"
     ]
    }
   ],
   "source": [
    "# Fit the model using the training data created above using 150 training iterations and a\n",
    "# validation split of 0.05\n",
    "hist = model.fit(X_train, y_train, batch_size=None, epochs=500, \n",
    "                 verbose=True,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creates a graph of the training loss/error and the validation loss/error as a function of the\n",
    "# number of training iterations performed.  This is useful to make sure the model is not \n",
    "# overtraining.\n",
    "# Get the data from the trained model and plot it\n",
    "for label in [\"loss\",\"val_loss\"]:\n",
    "    plt.plot(hist.history[label],label=label)\n",
    "# Label the x axis, the y axis, and add a title\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"The final validation loss: {}\".format(hist.history[\"val_loss\"][-1]))\n",
    "# Add a legend then show the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312334.1270889614\n"
     ]
    }
   ],
   "source": [
    "y_test = y_tot[dim:]\n",
    "y_pred = model.predict(X_tot[dim:])\n",
    "\n",
    "print(((y_test-y_pred)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rnn (x_known, y_known):\n",
    "    \"\"\"\n",
    "        Inputs: \n",
    "            x_known (a list or numpy array): the known x data, likely X_tot imported using\n",
    "                the data set.\n",
    "            y_known (a list or numpy array): the known y data, likely y_tot imported using\n",
    "                the data set.\n",
    "        Returns:\n",
    "            None.\n",
    "        Extrapolates from the training data to a complete data set using the trained recurrent\n",
    "        neural network.  Performs data analysis on the predicted data points and creates a graph\n",
    "        of the known data and the predicted data.\n",
    "    \"\"\"\n",
    "    # Segment off the training data from the known data\n",
    "    y_pred = y_known[:dim].tolist()\n",
    "    # Create the first point that will be used to predict the following point using the trained\n",
    "    # recurrent neural network.  In this case the first point contains the two points if the \n",
    "    # training data, so the first point that will be predicted is the first point to fall sequentially\n",
    "    # after the training data\n",
    "    next_input = np.array([[[y_known[dim-2]], [y_known[dim-1]]]])\n",
    "    # Save the last number in the prediction point for later use\n",
    "    last = [y_known[dim-1]]\n",
    "    # Loop until the predicted data set is the same length as the known data set.\n",
    "    for i in range (dim, len(y_known)):\n",
    "        # Predict the next point and add it to the predicted data set\n",
    "        next = model.predict(np.asarray(next_input))\n",
    "        y_pred.append(next[0][0])\n",
    "        # print the difference between the predicted point and the correspinding known point\n",
    "        print ('DIFF: ', next[0][0]-y_known[i])\n",
    "        # Create the point that will be uses to make a prediction on the next interation\n",
    "        next_input = np.array([[last, next[0]]], dtype=np.float64)\n",
    "        last = next\n",
    "    # Print the MSE of the predicted data and the known data.  This is a measure of how well the \n",
    "    # extrapolation worked.\n",
    "    print('MSE: ', np.square(np.subtract(y_known, y_pred)).mean())\n",
    "    # Save the predicted data set as a csv file for future use\n",
    "    #name = datatype + 'Predicted'+str(dim)+'.csv'\n",
    "    #np.savetxt(name, y_pred, delimiter=',')\n",
    "    # Plot both the known and the predicted data sets and add a legend\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x_known, y_known, label=\"true\", linewidth=3)\n",
    "    ax.plot(x_known, y_pred, 'g-.',label=\"predicted\", linewidth=4)\n",
    "    ax.legend()\n",
    "    # Create a semi-transparent red box to represent the training data\n",
    "    ax.axvspan(x_known[0], x_known[dim], alpha=0.25, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 1) for input Tensor(\"input_2:0\", shape=(None, 1, 1), dtype=float32), but it was called on an input with incompatible shape (None, 2, 1).\n",
      "DIFF:  1810970.8125816693\n",
      "DIFF:  2188468276.437337\n",
      "DIFF:  2642260523212.6704\n",
      "DIFF:  3190146820208733.0\n",
      "DIFF:  3.8516398096342036e+18\n",
      "DIFF:  4.650296503932742e+21\n",
      "DIFF:  5.614558247974168e+24\n",
      "DIFF:  6.778765240021009e+27\n",
      "DIFF:  8.184377023906616e+30\n",
      "DIFF:  9.881449081322623e+33\n",
      "MSE:  1.084923365898277e+66\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict the remaining points to finish the data set\n",
    "test_rnn(X_tot, y_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
